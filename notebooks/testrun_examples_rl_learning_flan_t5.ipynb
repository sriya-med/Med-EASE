{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474952a3a39e4dfb895decfb8dbdb670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51e922d86124f52baa79c5f23e04a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c5cfba6a344123b4c73b65907a92f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef0e2a2fe284e98896d5fe84d6a327d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a825ec0be04adfa9f86bceffd1fdc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1397 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fff606f51d948cd84d94c61bf97a1ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a884ea2db12740fba03c986e74085fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Expert', 'Simple', 'Annotation', 'sim', 'sentence_sim', 'compression', 'expert_fk_grade', 'expert_ari', 'layman_fk_grade', 'layman_ari', 'umls_expert', 'umls_layman', 'expert_terms', 'layman_terms', 'idx'],\n",
      "        num_rows: 1397\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['Expert', 'Simple', 'Annotation', 'sim', 'sentence_sim', 'compression', 'expert_fk_grade', 'expert_ari', 'layman_fk_grade', 'layman_ari', 'umls_expert', 'umls_layman', 'expert_terms', 'layman_terms', 'idx'],\n",
      "        num_rows: 196\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Expert', 'Simple', 'Annotation', 'sim', 'sentence_sim', 'compression', 'expert_fk_grade', 'expert_ari', 'layman_fk_grade', 'layman_ari', 'umls_expert', 'umls_layman', 'expert_terms', 'layman_terms', 'idx'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n",
      "Train sample:\n",
      " {'Expert': '75-90 % of the affected people have mild intellectual disability.', 'Simple': \"People with syndromic intellectual disabilities may have a `` typical look. ''\", 'Annotation': \"<del>75-90 % of the</del> <rep>affected people  have mild intellectual disability.<by>People with syndromic intellectual disabilities</rep> <ins>may have a `` typical look. ''</ins>\", 'sim': 0.48951049, 'sentence_sim': 0.639872432, 'compression': 1.2, 'expert_fk_grade': 12.7, 'expert_ari': 12.4, 'layman_fk_grade': 13.1, 'layman_ari': 15.1, 'umls_expert': \"[[{'start': 41, 'end': 64, 'ngram': 'intellectual disability', 'term': 'intellectual disability', 'cui': 'C3714756', 'similarity': 1.0, 'semtypes': {'T048'}, 'preferred': 1, 'preferred_term': None}, {'start': 41, 'end': 64, 'ngram': 'intellectual disability', 'term': 'Intellectual disability', 'cui': 'C3714756', 'similarity': 0.9090909090909091, 'semtypes': {'T048'}, 'preferred': 0, 'preferred_term': None}, {'start': 41, 'end': 64, 'ngram': 'intellectual disability', 'term': 'Intellectual disability, mild', 'cui': 'C0026106', 'similarity': 0.7142857142857143, 'semtypes': {'T048'}, 'preferred': 0, 'preferred_term': None}]]\", 'umls_layman': \"[[{'start': 35, 'end': 47, 'ngram': 'disabilities', 'term': 'disabilities', 'cui': 'C0231170', 'similarity': 1.0, 'semtypes': {'T033'}, 'preferred': 1, 'preferred_term': None}], [{'start': 62, 'end': 69, 'ngram': 'typical', 'term': 'Atypical', 'cui': 'C0741302', 'similarity': 0.8333333333333334, 'semtypes': {'T033'}, 'preferred': 0, 'preferred_term': None}], [{'start': 22, 'end': 34, 'ngram': 'intellectual', 'term': 'intellect', 'cui': 'C2981149', 'similarity': 0.7, 'semtypes': {'T041'}, 'preferred': 1, 'preferred_term': None}], [{'start': 12, 'end': 21, 'ngram': 'syndromic', 'term': 'Nonsyndromic', 'cui': 'C2677304', 'similarity': 0.7, 'semtypes': {'T033'}, 'preferred': 1, 'preferred_term': None}]]\", 'expert_terms': \"['intellectual disability']\", 'layman_terms': \"['disabilities', 'intellect', 'Atypical', 'Nonsyndromic']\", 'idx': 0}\n"
     ]
    }
   ],
   "source": [
    "#fine tune model on med-easi dataset with flan-t5-base to get baseline model\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "import torch\n",
    "\n",
    "#load Med-EASi dataset\n",
    "ds = load_dataset(\"cbasu/Med-EASi\")\n",
    "print(ds)\n",
    "print(\"Train sample:\\n\", ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e8f4a2b31a45cb989a03e66c6388c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19536cca7fc140cdbaa861e94dd3c13e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a39e1ed4134992b17eda33faa952bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7eb259c5034993bfa1d7c3e2d3d499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924bd7af8f954b6bbfc4a4c49e637ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1397 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d622659d42b5404eb9b6d071c7e020cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76aa99b5d0e943eaac226bd8d843e036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1397\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 196\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess(batch):\n",
    "    inputs = batch[\"Expert\"]\n",
    "    targets = batch[\"Simple\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        text_target=targets,\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "cols_to_remove = ds[\"train\"].column_names\n",
    "\n",
    "tokenized_ds = ds.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=cols_to_remove,\n",
    ")\n",
    "\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e32df7adbf4e0f9ad229d5b83fe012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b30f499e254f93a6ee13fb6867adca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3ed005c16242fa977c06570af8e6fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▄▇█</td></tr><tr><td>train/global_step</td><td>▁▄▇█</td></tr><tr><td>train/grad_norm</td><td>▄▁█</td></tr><tr><td>train/learning_rate</td><td>▁▄█</td></tr><tr><td>train/loss</td><td>█▆▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.88492</td></tr><tr><td>eval/runtime</td><td>1.1505</td></tr><tr><td>eval/samples_per_second</td><td>170.368</td></tr><tr><td>eval/steps_per_second</td><td>21.731</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>175</td></tr><tr><td>train/grad_norm</td><td>3.75387</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.5983</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">flan-t5-base-rl-baseline</strong> at: <a href='https://wandb.ai/vsonnaku-george-mason-university/Med-EASE/runs/g4ai959m' target=\"_blank\">https://wandb.ai/vsonnaku-george-mason-university/Med-EASE/runs/g4ai959m</a><br> View project at: <a href='https://wandb.ai/vsonnaku-george-mason-university/Med-EASE' target=\"_blank\">https://wandb.ai/vsonnaku-george-mason-university/Med-EASE</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251201_180441-g4ai959m/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251201_180512-cy6w0qcf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vsonnaku-george-mason-university/Med-EASE/runs/cy6w0qcf' target=\"_blank\">flan-t5-base-rl-baseline</a></strong> to <a href='https://wandb.ai/vsonnaku-george-mason-university/Med-EASE' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vsonnaku-george-mason-university/Med-EASE' target=\"_blank\">https://wandb.ai/vsonnaku-george-mason-university/Med-EASE</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vsonnaku-george-mason-university/Med-EASE/runs/cy6w0qcf' target=\"_blank\">https://wandb.ai/vsonnaku-george-mason-university/Med-EASE/runs/cy6w0qcf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-661495471.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='875' max='875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [875/875 02:39, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.486900</td>\n",
       "      <td>1.862019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.549200</td>\n",
       "      <td>1.821185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.476000</td>\n",
       "      <td>1.794517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.337900</td>\n",
       "      <td>1.778186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.296900</td>\n",
       "      <td>1.762969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best checkpoint path: checkpoints/flan_t5_baseline/checkpoint-875\n",
      "Best metric 1.7629690170288086\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"Med-EASE\", name=\"flan-t5-base-rl-baseline\")\n",
    "\n",
    "#using best args from experiment\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"checkpoints/flan_t5_baseline\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=5,\n",
    "    warmup_steps=1000,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    tpu_num_cores=8,\n",
    "    predict_with_generate=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "best_ckpt = trainer.state.best_model_checkpoint\n",
    "best_metric = trainer.state.best_metric\n",
    "\n",
    "print(\"Best checkpoint path:\", best_ckpt)\n",
    "print(\"Best metric\", best_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▅▃▂▁</td></tr><tr><td>eval/runtime</td><td>▁▅▄█▄</td></tr><tr><td>eval/samples_per_second</td><td>█▄▅▁▅</td></tr><tr><td>eval/steps_per_second</td><td>█▄▅▁▅</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▃▃▄▄▄▄▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▃▃▄▄▄▄▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▅▄▅▅▄▁▇▂▂▃▃▆▂█▂▁▃</td></tr><tr><td>train/learning_rate</td><td>▁▁▂▂▃▃▄▄▄▅▅▆▆▇▇██</td></tr><tr><td>train/loss</td><td>█▇▅▆▅▆▇▃▆▅▄▄▄▂▂▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.76297</td></tr><tr><td>eval/runtime</td><td>1.1348</td></tr><tr><td>eval/samples_per_second</td><td>172.714</td></tr><tr><td>eval/steps_per_second</td><td>22.03</td></tr><tr><td>total_flos</td><td>691261209879552.0</td></tr><tr><td>train/epoch</td><td>5</td></tr><tr><td>train/global_step</td><td>875</td></tr><tr><td>train/grad_norm</td><td>2.63092</td></tr><tr><td>train/learning_rate</td><td>3e-05</td></tr><tr><td>train/loss</td><td>1.2969</td></tr><tr><td>+4</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">flan-t5-base-rl-baseline</strong> at: <a href='https://wandb.ai/vsonnaku-george-mason-university/Med-EASE/runs/cy6w0qcf' target=\"_blank\">https://wandb.ai/vsonnaku-george-mason-university/Med-EASE/runs/cy6w0qcf</a><br> View project at: <a href='https://wandb.ai/vsonnaku-george-mason-university/Med-EASE' target=\"_blank\">https://wandb.ai/vsonnaku-george-mason-university/Med-EASE</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251201_180512-cy6w0qcf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import textstat\n",
    "import ast\n",
    "\n",
    "def readability_reward(text: str, min_grade: float = 5.0, max_grade: float = 15.0) -> float:\n",
    "    \"\"\"\n",
    "    Return score between 0 and 1, where 1 = easiest to read, 0 = very hard\n",
    "    map Flesch-Kincaid grade from [min_grade, max_grade] to [1,0].\n",
    "    \"\"\"\n",
    "    grade = textstat.flesch_kincaid_grade(text)\n",
    "    if math.isnan(grade):\n",
    "        return 0.0\n",
    "\n",
    "    grade = max(min_grade, min(max_grade, grade))\n",
    "    score = 1.0 - (grade - min_grade) / (max_grade - min_grade)\n",
    "    return score\n",
    "\n",
    "\n",
    "def parse_expert_terms(s: str):\n",
    "    \"\"\"\n",
    "    expert_terms is stored like \"['intellectual disability']\" within med-easi dataset.\n",
    "    parse it into a list and convert to lowercase\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return []\n",
    "    try:\n",
    "        terms = ast.literal_eval(s)\n",
    "        if isinstance(terms, list):\n",
    "            return [t.lower() for t in terms]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return []\n",
    "\n",
    "def entity_reward(prediction: str, expert_terms_str: str) -> float:\n",
    "    \"\"\"\n",
    "    return fraction of expert terms present in the prediction output.\n",
    "    \"\"\"\n",
    "    terms = parse_expert_terms(expert_terms_str)\n",
    "    if not terms:\n",
    "        return 1.0\n",
    "\n",
    "    pred_lower = prediction.lower()\n",
    "    present = sum(int(term in pred_lower) for term in terms)\n",
    "    return present / len(terms)\n",
    "\n",
    "def noncopy_reward(prediction: str, expert: str) -> float:\n",
    "    \"\"\"\n",
    "    reward b/w 0 and 1, where 1 = very different from Expert field,\n",
    "    0 = identical / heavily overlapping from Expert field.\n",
    "    \"\"\"\n",
    "    pred_tokens = set(prediction.lower().split())\n",
    "    expert_tokens = set(expert.lower().split())\n",
    "    if not pred_tokens or not expert_tokens:\n",
    "        return 0.0\n",
    "    jaccard = len(pred_tokens & expert_tokens) / len(pred_tokens | expert_tokens) #use jaccard similarity\n",
    "    #high overlap ==> low reward\n",
    "    return 1.0 - jaccard\n",
    "\n",
    "#favor outputs shorter than expert field\n",
    "def length_reward(prediction: str, expert: str) -> float:\n",
    "    \"\"\"\n",
    "    Reward b/w 0 and 1: 1 if prediction is much shorter than expert,\n",
    "    decreasing as it approaches or exceeds expert length.\n",
    "    \"\"\"\n",
    "    len_pred = len(prediction.split())\n",
    "    len_expert = len(expert.split())\n",
    "    if len_expert == 0:\n",
    "        return 0.0\n",
    "\n",
    "    ratio = len_pred / len_expert  #want this < 1 ideally\n",
    "    if ratio >= 1.0:\n",
    "        return 0.0\n",
    "    #if ratio <= 0.5 -> ~1.0, if ratio approaching 1.0 -> 0.0\n",
    "    return 1.0 - (ratio - 0.5) / 0.5 if ratio > 0.5 else 1.0\n",
    "\n",
    "\n",
    "def combined_reward(prediction, expert, expert_terms_str, w_read=0.40, w_ent=0.15, w_noncopy=0.35, w_len=0.10):\n",
    "    r_read = readability_reward(prediction)\n",
    "    r_ent  = entity_reward(prediction, expert_terms_str)\n",
    "    r_nc   = noncopy_reward(prediction, expert)\n",
    "    r_len  = length_reward(prediction, expert)\n",
    "\n",
    "    return (\n",
    "        w_read * r_read +\n",
    "        w_ent  * r_ent +\n",
    "        w_noncopy * r_nc +\n",
    "        w_len * r_len\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def generate_with_reranking(batch, num_candidates = 4):\n",
    "    experts = batch[\"Expert\"]\n",
    "    expert_terms_list = batch[\"expert_terms\"]\n",
    "\n",
    "    preds_rl = []\n",
    "\n",
    "    model.eval()\n",
    "    for expert, expert_terms_str in zip(experts, expert_terms_list):\n",
    "        inputs = tokenizer(\n",
    "            expert,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=256,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.9,\n",
    "                temperature=1.2,\n",
    "                num_return_sequences=num_candidates,\n",
    "            )\n",
    "\n",
    "        candidates = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        best_cand = None\n",
    "        best_reward = -1.0\n",
    "        for cand in candidates:\n",
    "            r = combined_reward(\n",
    "                prediction=cand,\n",
    "                expert=expert,\n",
    "                expert_terms_str=expert_terms_str,\n",
    "            )\n",
    "            if r > best_reward:\n",
    "                best_reward = r\n",
    "                best_cand = cand\n",
    "\n",
    "        preds_rl.append(best_cand)\n",
    "\n",
    "    batch[\"pred_rl\"] = preds_rl\n",
    "    return batch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 - Original:\n",
      "The patient was diagnosed with congestive heart failure and exhibited pulmonary oedema requiring diuretics and oxygen supplementation.\n",
      "\n",
      "Example 1 - Simplified:\n",
      "This patient developed hyperthyroidism and is at increased risk for congestive heart failure.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Example 2 - Original:\n",
      "Administration of 5-fluorouracil can lead to myelosuppression and mucositis; dose adjustments may be necessary in elderly patients.\n",
      "\n",
      "Example 2 - Simplified:\n",
      "5-fluorouracil can lead to meelosuppression and mucositis. Some elderly people may benefit from taking this drug.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Example 3 - Original:\n",
      "Hyponatraemia in this context is likely due to SIADH caused by the small-cell lung carcinoma, necessitating fluid restriction and close monitoring of sodium levels.\n",
      "\n",
      "Example 3 - Simplified:\n",
      "This may lead to SIADH, resulting in very low blood sugar.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Example 4 - Original:\n",
      "The MRI demonstrates a hyperintense lesion on T2-weighted images consistent with demyelination in the periventricular white matter.\n",
      "\n",
      "Example 4 - Simplified:\n",
      "The MRI showed an hyperintense lesion in the periventricular white matter (T2-weighted images); this is consistent with demyelination in the periventricular white matter.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Example 5 - Original:\n",
      "The patient presents with chronic kidney disease stage 3b, eGFR 35 mL/min/1.73m2, and requires medication dose review to avoid nephrotoxicity.\n",
      "\n",
      "Example 5 - Simplified:\n",
      "The person is suffering from stage 3b, and has the following conditions: acute renal failure - kidney disease ( eGFR 35 mL/min/1.73m2; hyperethoxidemia) - a low GFR which is 35mL/min ( eGFR 3 mL/min); rapid renal failure ( eGFR 35 mL/min; severe renal failure); acute renal failure ( eGFR 35 mL/min/1.73m2; renal dysfunction) - poor kidney function; renal failure ( eGFR 35 mL/min); renal failure ( eGFR 35 mL/min/1.73m2).\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    \"The patient was diagnosed with congestive heart failure and exhibited pulmonary oedema requiring diuretics and oxygen supplementation.\",\n",
    "    \"Administration of 5-fluorouracil can lead to myelosuppression and mucositis; dose adjustments may be necessary in elderly patients.\",\n",
    "    \"Hyponatraemia in this context is likely due to SIADH caused by the small-cell lung carcinoma, necessitating fluid restriction and close monitoring of sodium levels.\",\n",
    "    \"The MRI demonstrates a hyperintense lesion on T2-weighted images consistent with demyelination in the periventricular white matter.\",\n",
    "    \"The patient presents with chronic kidney disease stage 3b, eGFR 35 mL/min/1.73m2, and requires medication dose review to avoid nephrotoxicity.\"\n",
    "]\n",
    "\n",
    "for i, text in enumerate(examples, 1):\n",
    "    # generate_with_reranking expects a batch, so we create one for a single example\n",
    "    single_example_batch = {\n",
    "        \"Expert\": [text],\n",
    "        \"expert_terms\": [\"[]\"] # No expert terms for these custom examples\n",
    "    }\n",
    "\n",
    "    # The function returns a dictionary with 'pred_rl'\n",
    "    result = generate_with_reranking(single_example_batch)\n",
    "    simplified_text = result[\"pred_rl\"][0]\n",
    "\n",
    "    # Print original and simplified outputs\n",
    "    print(f\"Example {i} - Original:\\n{text}\\n\")\n",
    "    print(f\"Example {i} - Simplified:\\n{simplified_text}\\n\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

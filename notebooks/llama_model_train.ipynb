{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fadce62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#This notebook fine-tunes Llama-3.1-8B-Instruct to explain medical diagnoses to patients using the Med-EASi dataset.\n",
    "\n",
    "!pip install -q \"transformers>=4.43.0\" peft bitsandbytes accelerate datasets evaluate wandb huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38f4fdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Hugging Face token (with access to Llama-3.1-8B): ··········\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from getpass import getpass\n",
    "\n",
    "HF_TOKEN = getpass(\"Enter your Hugging Face token (with access to Llama-3.1-8B): \")\n",
    "login(HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb231dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import wandb\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3f91abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6cc18147b4b4fb5ac76d66ac9bdc5ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baeb8a6f12524a7987187d7986362b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1151c15a4cb42a0ab95fa180f6a9317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a0037484e64c58ac5e211791b82a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d21287991f498899697623b909ad80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1397 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6dd928c5d3d479395c7d3f7bc3b0719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1adeaaa7ef96474583e9feee4bd5291c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Expert', 'Simple', 'Annotation', 'sim', 'sentence_sim', 'compression', 'expert_fk_grade', 'expert_ari', 'layman_fk_grade', 'layman_ari', 'umls_expert', 'umls_layman', 'expert_terms', 'layman_terms', 'idx'],\n",
      "        num_rows: 1397\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['Expert', 'Simple', 'Annotation', 'sim', 'sentence_sim', 'compression', 'expert_fk_grade', 'expert_ari', 'layman_fk_grade', 'layman_ari', 'umls_expert', 'umls_layman', 'expert_terms', 'layman_terms', 'idx'],\n",
      "        num_rows: 196\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Expert', 'Simple', 'Annotation', 'sim', 'sentence_sim', 'compression', 'expert_fk_grade', 'expert_ari', 'layman_fk_grade', 'layman_ari', 'umls_expert', 'umls_layman', 'expert_terms', 'layman_terms', 'idx'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n",
      "Train sample:\n",
      " {'Expert': '75-90 % of the affected people have mild intellectual disability.', 'Simple': \"People with syndromic intellectual disabilities may have a `` typical look. ''\", 'Annotation': \"<del>75-90 % of the</del> <rep>affected people  have mild intellectual disability.<by>People with syndromic intellectual disabilities</rep> <ins>may have a `` typical look. ''</ins>\", 'sim': 0.48951049, 'sentence_sim': 0.639872432, 'compression': 1.2, 'expert_fk_grade': 12.7, 'expert_ari': 12.4, 'layman_fk_grade': 13.1, 'layman_ari': 15.1, 'umls_expert': \"[[{'start': 41, 'end': 64, 'ngram': 'intellectual disability', 'term': 'intellectual disability', 'cui': 'C3714756', 'similarity': 1.0, 'semtypes': {'T048'}, 'preferred': 1, 'preferred_term': None}, {'start': 41, 'end': 64, 'ngram': 'intellectual disability', 'term': 'Intellectual disability', 'cui': 'C3714756', 'similarity': 0.9090909090909091, 'semtypes': {'T048'}, 'preferred': 0, 'preferred_term': None}, {'start': 41, 'end': 64, 'ngram': 'intellectual disability', 'term': 'Intellectual disability, mild', 'cui': 'C0026106', 'similarity': 0.7142857142857143, 'semtypes': {'T048'}, 'preferred': 0, 'preferred_term': None}]]\", 'umls_layman': \"[[{'start': 35, 'end': 47, 'ngram': 'disabilities', 'term': 'disabilities', 'cui': 'C0231170', 'similarity': 1.0, 'semtypes': {'T033'}, 'preferred': 1, 'preferred_term': None}], [{'start': 62, 'end': 69, 'ngram': 'typical', 'term': 'Atypical', 'cui': 'C0741302', 'similarity': 0.8333333333333334, 'semtypes': {'T033'}, 'preferred': 0, 'preferred_term': None}], [{'start': 22, 'end': 34, 'ngram': 'intellectual', 'term': 'intellect', 'cui': 'C2981149', 'similarity': 0.7, 'semtypes': {'T041'}, 'preferred': 1, 'preferred_term': None}], [{'start': 12, 'end': 21, 'ngram': 'syndromic', 'term': 'Nonsyndromic', 'cui': 'C2677304', 'similarity': 0.7, 'semtypes': {'T033'}, 'preferred': 1, 'preferred_term': None}]]\", 'expert_terms': \"['intellectual disability']\", 'layman_terms': \"['disabilities', 'intellect', 'Atypical', 'Nonsyndromic']\", 'idx': 0}\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"cbasu/Med-EASi\")\n",
    "print(ds)\n",
    "print(\"Train sample:\\n\", ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d57efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d535eb2e4b7455a8b07669be9e9bdd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82301421766945dea911a0eb0ff7944f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521e1a17eb5c45d79c143bfe9c5cf805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62a6cf7a01e45d1ad3000fe6417e49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2450438fa5b6448d9fae3283571d7819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4384ab9ad1b4d548665b16f46e911a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40edbbc661e045b8a5ea71ee9d8909a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4f9364b3624418874c35deb73026a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094bb5557f7642ebad9d0b7f6b4f6a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998e951752e04a3081f6ed89b4862209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab26c6486274435835917181f344b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d880bc4b664ffda8943209b385f60a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast=True,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6d7b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfadfcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24dd7a174df4315900dd786fdeaab58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1397 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c232e210390459ebebbc6984ef81104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd36c338aa1b4f169280b61ae9eeb069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1397\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 196\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH = 256 #change later if needed\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a medical explainer. Your job is to explain medical diagnoses \"\n",
    "    \"in clear, patient-friendly English. Use short sentences, avoid medical \"\n",
    "    \"jargon when possible, and do not add information that is not in the \"\n",
    "    \"original text.\\n\\n\"\n",
    ")\n",
    "\n",
    "def build_training_prompt(expert_text: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Explain the following medical diagnosis to a patient in simple, \"\n",
    "                \"everyday language:\\n\\n\"\n",
    "                f\"{expert_text}\\n\\n\"\n",
    "                \"Patient-friendly explanation:\"\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    experts = batch[\"Expert\"]\n",
    "    simples = batch[\"Simple\"]\n",
    "\n",
    "    prompts = [build_training_prompt(e) for e in experts]\n",
    "    targets = simples\n",
    "\n",
    "    full_texts = [p + t + tokenizer.eos_token for p, t in zip(prompts, targets)]\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        full_texts,\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    labels = []\n",
    "    attention_masks = tokenized[\"attention_mask\"]\n",
    "\n",
    "    for i, full_ids in enumerate(input_ids):\n",
    "        #tokenize prompt to get number of prompt tokens\n",
    "        prompt_ids = tokenizer(\n",
    "            prompts[i],\n",
    "            add_special_tokens=False,\n",
    "            max_length=MAX_LENGTH,\n",
    "            truncation=True,\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        label_ids = full_ids.copy()\n",
    "        prompt_len = min(len(prompt_ids), len(label_ids))\n",
    "\n",
    "        #we only want loss on answer tokens, so mask out the prompt tokens\n",
    "        label_ids[:prompt_len] = [-100] * prompt_len\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "cols_to_remove = ds[\"train\"].column_names\n",
    "\n",
    "tokenized_ds = ds.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=cols_to_remove,\n",
    ")\n",
    "\n",
    "tokenized_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce58f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(features):\n",
    "    batch = {}\n",
    "    for key in [\"input_ids\", \"attention_mask\", \"labels\"]:\n",
    "        batch[key] = torch.stack(\n",
    "            [torch.tensor(f[key]) for f in features]\n",
    "        )\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae947a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">llama3_1_8b_model</strong> at: <a href='https://wandb.ai/sriyamed1-none/med-easi-llama3/runs/w0xhpzs1' target=\"_blank\">https://wandb.ai/sriyamed1-none/med-easi-llama3/runs/w0xhpzs1</a><br> View project at: <a href='https://wandb.ai/sriyamed1-none/med-easi-llama3' target=\"_blank\">https://wandb.ai/sriyamed1-none/med-easi-llama3</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251201_025659-w0xhpzs1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251201_025725-jyxrahyb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sriyamed1-none/med-easi-llama3/runs/jyxrahyb' target=\"_blank\">llama3_1_8b_model</a></strong> to <a href='https://wandb.ai/sriyamed1-none/med-easi-llama3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sriyamed1-none/med-easi-llama3' target=\"_blank\">https://wandb.ai/sriyamed1-none/med-easi-llama3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sriyamed1-none/med-easi-llama3/runs/jyxrahyb' target=\"_blank\">https://wandb.ai/sriyamed1-none/med-easi-llama3/runs/jyxrahyb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"med-easi-llama3\",\n",
    "    name=\"llama3_1_8b_model\",\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"checkpoints/llama3_1_8b_med_easi\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.05,\n",
    "    logging_steps=20,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=[\"wandb\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2497fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-1526134375.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [700/700 48:01, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.226400</td>\n",
       "      <td>0.376193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.133900</td>\n",
       "      <td>0.403679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ee8619",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = getpass(\"Enter token with write access to upload model to HF repo\")\n",
    "login(HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d60dc2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85ef6f67bc540a891a112cfabdaa62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36934e4fba524007bc2e83b98947d507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cae9194253742348c97418039f5e812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...adapter_model.safetensors:   1%|          |  555kB / 83.9MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229a038d355240a5ae511611a766a4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611aae740165440790f22c0e7ee5ec6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398ca3499d184404ab97a49e70bfa739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0bae4a398a541398a038f5a3f14baa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...mpxkz5ltdl/tokenizer.json: 100%|##########| 17.2MB / 17.2MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/smedara/llama3-med-easi-explainer/commit/f1180ff60d998d8e1d2235b9de5b8a503c38bd6b', commit_message='Upload tokenizer', commit_description='', oid='f1180ff60d998d8e1d2235b9de5b8a503c38bd6b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/smedara/llama3-med-easi-explainer', endpoint='https://huggingface.co', repo_type='model', repo_id='smedara/llama3-med-easi-explainer'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"smedara/llama3-med-easi-explainer\" #REMINDER: change to your HF repo ID\n",
    "model.push_to_hub(repo_id=repo_id, private=True)\n",
    "tokenizer.push_to_hub(repo_id)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

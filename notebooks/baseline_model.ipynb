{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66f67c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sriya/Desktop/College/CS_678/final_project/Med-EASE/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Expert', 'Simple', 'Annotation', 'sim', 'sentence_sim', 'compression', 'expert_fk_grade', 'expert_ari', 'layman_fk_grade', 'layman_ari', 'umls_expert', 'umls_layman', 'expert_terms', 'layman_terms', 'idx'],\n",
      "        num_rows: 1397\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['Expert', 'Simple', 'Annotation', 'sim', 'sentence_sim', 'compression', 'expert_fk_grade', 'expert_ari', 'layman_fk_grade', 'layman_ari', 'umls_expert', 'umls_layman', 'expert_terms', 'layman_terms', 'idx'],\n",
      "        num_rows: 196\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Expert', 'Simple', 'Annotation', 'sim', 'sentence_sim', 'compression', 'expert_fk_grade', 'expert_ari', 'layman_fk_grade', 'layman_ari', 'umls_expert', 'umls_layman', 'expert_terms', 'layman_terms', 'idx'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n",
      "Train dataset sample: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Expert': '75-90 % of the affected people have mild intellectual disability.',\n",
       " 'Simple': \"People with syndromic intellectual disabilities may have a `` typical look. ''\",\n",
       " 'Annotation': \"<del>75-90 % of the</del> <rep>affected people  have mild intellectual disability.<by>People with syndromic intellectual disabilities</rep> <ins>may have a `` typical look. ''</ins>\",\n",
       " 'sim': 0.48951049,\n",
       " 'sentence_sim': 0.639872432,\n",
       " 'compression': 1.2,\n",
       " 'expert_fk_grade': 12.7,\n",
       " 'expert_ari': 12.4,\n",
       " 'layman_fk_grade': 13.1,\n",
       " 'layman_ari': 15.1,\n",
       " 'umls_expert': \"[[{'start': 41, 'end': 64, 'ngram': 'intellectual disability', 'term': 'intellectual disability', 'cui': 'C3714756', 'similarity': 1.0, 'semtypes': {'T048'}, 'preferred': 1, 'preferred_term': None}, {'start': 41, 'end': 64, 'ngram': 'intellectual disability', 'term': 'Intellectual disability', 'cui': 'C3714756', 'similarity': 0.9090909090909091, 'semtypes': {'T048'}, 'preferred': 0, 'preferred_term': None}, {'start': 41, 'end': 64, 'ngram': 'intellectual disability', 'term': 'Intellectual disability, mild', 'cui': 'C0026106', 'similarity': 0.7142857142857143, 'semtypes': {'T048'}, 'preferred': 0, 'preferred_term': None}]]\",\n",
       " 'umls_layman': \"[[{'start': 35, 'end': 47, 'ngram': 'disabilities', 'term': 'disabilities', 'cui': 'C0231170', 'similarity': 1.0, 'semtypes': {'T033'}, 'preferred': 1, 'preferred_term': None}], [{'start': 62, 'end': 69, 'ngram': 'typical', 'term': 'Atypical', 'cui': 'C0741302', 'similarity': 0.8333333333333334, 'semtypes': {'T033'}, 'preferred': 0, 'preferred_term': None}], [{'start': 22, 'end': 34, 'ngram': 'intellectual', 'term': 'intellect', 'cui': 'C2981149', 'similarity': 0.7, 'semtypes': {'T041'}, 'preferred': 1, 'preferred_term': None}], [{'start': 12, 'end': 21, 'ngram': 'syndromic', 'term': 'Nonsyndromic', 'cui': 'C2677304', 'similarity': 0.7, 'semtypes': {'T033'}, 'preferred': 1, 'preferred_term': None}]]\",\n",
       " 'expert_terms': \"['intellectual disability']\",\n",
       " 'layman_terms': \"['disabilities', 'intellect', 'Atypical', 'Nonsyndromic']\",\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"cbasu/Med-EASi\")\n",
    "\n",
    "print(ds)\n",
    "print(\"Train dataset sample: \")\n",
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40a21cd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[32m      4\u001b[39m tokenizer = AutoTokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mgoogle/flan-t5-base\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocess_and_tokenizer\u001b[39m(batch):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "\n",
    "def preprocess_and_tokenizer(batch):\n",
    "    inputs = batch['Expert']\n",
    "    targets = batch['Simple']\n",
    "\n",
    "    model_inputs = tokenizer(inputs, truncation=True, max_length=256)\n",
    "    labels = tokenizer(text_target=targets, truncation=True, max_length=256)[\"input_ids\"]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "col_names = ds[\"train\"].column_names\n",
    "tokenized_ds = ds.map(preprocess_and_tokenizer, batched=True, remove_columns=col_names)\n",
    "\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0759db8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
